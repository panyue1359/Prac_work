---
title: 随机森林
---

- 随机森林/别名：random forest, RF
- 随机森林/定义
- 随即森林/异同
	- 集成学习和随机森林
	  id:: yJj3p-Uy7
	  id:: 29ff39ec-4336-4552-9693-a7281ac5bd7a
		- Bagging集成学习决策树增加随机性
- 随机森林/应用
	- 调参
		- 根据 [scikit-learn随机森林调参小结 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/6160412.html) 确定调参顺序
		  id:: 6145bb3f-38c5-4a9e-bd0e-fdfcbedba05d
		- [sklearn对随机森林调参的建议](https://hyp.is/9wB_zjAKEeyiJKcPx5SlzQ/scikit-learn.org/stable/modules/ensemble.html)
		  :LOGBOOK:
		  CLOCK: [2021-10-18 Mon 20:03]
		  :END:
		- 网格搜索结果展示代码参考 [Python机器学习库sklearn网格搜索与交叉验证_Young_618-CSDN博客_sklearn 网格搜索](https://blog.csdn.net/cymy001/article/details/78578665)
		  id:: 61449dfe-c05f-4ce6-a5bd-74cb94a6c30b
			- ``` python
			  print('GridSearchCV交叉验证网格搜索字典获得的最好参数组合',
			  gsearch1.best_params_)
			  print('GridSearchCV交叉验证网格搜索获得的最好估计器,在训练验证集上没做交叉验证的得分',
			  gsearch1.score(X_trainval,y_trainval))
			  print('GridSearchCV交叉验证网格搜索获得的最好估计器,在**集上做交叉验证的平均得分',
			  gsearch1.best_score_)
			  print('GridSearchCV交叉验证网格搜索获得的最好估计器,在测试集上的得分',
			  gsearch1.score(X_test, y_test))
			  
			  ```
		-
	- ((4f92dc7b-9df6-4972-b309-f33fd0a0e9c3))
		- 如果不控制随机数种子，那么随机森林特征重要性的排名可能会改变；如果改变很大，说明不同特征对响应变量的影响差别不大 [python feature selection feature importance method from sklearn.ensemble gives inconsistent results in multiple runs - Stack Overflow](https://stackoverflow.com/questions/57664705/python-feature-selection-feature-importance-method-from-sklearn-ensemble-gives-i)
		- "Prediction from pure-prediction algorithm is accurate, not so much because of a specific/particular variable, but because of the interim non-linear transformation which includes many variables." 如果特征之间存在非线性相关性，那么去除其中一些变量不会损害模型表现 [Random forest importance measures are NOT important](https://eranraviv.com/random-forest-importance-measures-are-not-important/)
		- 一个比较直观的例子是特征A是面积，特征B和C是长和宽，那么即使去掉特征A，也可以通过特征B和C凑出特征A [python - How to interpret feature importance for ensemble methods? - Stack Overflow](https://stackoverflow.com/questions/43637662/how-to-interpret-feature-importance-for-ensemble-methods)
		- 基于不纯度降低的特征重要性只能说明模型使用各个特征的次数，“对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。”也就是说，随机森林特征重要性排序靠前的特征确实是重要特征，但排序靠后不一定不重要，即随机森林倾向于低估特征的重要性。[budomo a 特征选择——高级部分（线性模型正则化，随机森林、基于顶层模型的方法实现特征选择）](https://zhuanlan.zhihu.com/p/141704199)
	- ((af533a86-518c-4c0d-bf0a-ae6ceb704f9f))
		- “Main takeaway from the post: don’t use those importance scores plots, because they are simply misleading. Those importance plots are simply a wrong turn taken by our human tendency to look for reason, whether it’s there or it’s not there.” [Random forest importance measures are NOT important](https://eranraviv.com/random-forest-importance-measures-are-not-important/)
- 随机森林/文献
	- [scikit-learn随机森林调参小结 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/6160412.html)