- ## 基金项目
  collapsed:: true
	- [[学术研究与交流]]
	  collapsed:: true
		- ->[如何查询导师的国家自然科学基金项目以及书写结题报告：考研｜考博_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1MB4y137vW/?spm_id_from=333.999.0.0&vd_source=fc591008a48bd1bb56b8e3ba9a7c2202)
- ## 深度学习可以避免维度灾难
  collapsed:: true
	- ((6391ad15-1a2b-4dcf-883a-7190709d124e))
	  collapsed:: true
		- collapsed:: true
		  >参考
			- [王轩泽：AI为金属材料科学的研究和产业化带来的一些变化_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1za41197Jh/?spm_id_from=333.999.0.0&vd_source=fc591008a48bd1bb56b8e3ba9a7c2202)
			- [【郭永怀力学进展讲座】机器学习与物理模型 by 鄂维南 院士_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Wt4y1C7oG/?spm_id_from=333.337.search-card.all.click&vd_source=fc591008a48bd1bb56b8e3ba9a7c2202)
			- ((6389d38a-647f-4a6b-9854-42f32e961039))
- 王轩泽对高通量实验+数据驱动机器学习的未来进行了畅想，没有实质内容。
- 以下是鄂维南讲座内容。
- ad hoc：表示精心调试出来的、只适用于当前情况的方法。[机器学习中的 ad hoc 是什么意思？ - 知乎](https://www.zhihu.com/question/381100372/answer/1097483738)
- CoD：鄂维南认为维度灾难是当前数值模拟方法的一个瓶颈
- 鄂维南认为多尺度模型失败的一个原因是其本身不够可靠
- 鄂维南分析了神经网络和蒙特卡洛方法的关系，说明其可以避免CoD
- 借助机器学习将微观模型拓展为较为可靠的宏观模型
- 鄂维南将on the fly的机器学习称为concurrent learning，先收集数据然后建模的机器学习称为sequential learning
- 鄂维南团队提出了ELT框架：Exploration（采样）、Labeling（计算）和Training（训练），用于计算机器学习势能函数，并推出软件DeepMD
- ## spyct
	- ((64468a02-5425-4d78-9616-62e848fbdc27))
- [Semi-supervised oblique predictive clustering trees](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8101547/)：提供了[源码](https://gitlab.com/TStepi/spyct)
  collapsed:: true
	- 无标签数据可以影响聚类变量(clustering variables)，通过参数$\omega$控制
	- 代码中默认clustering_data是target_data，即参数$\omega$取1，无标签数据完全不影响模型构建
	- 原文提到使用参数 $$\omega$$ 控制无标签数据对模型的影响，但 $$\omega$$ 需要针对每个数据集动态选择，文章指出这是该方法的主要缺点
	- gitlab代码中，没有提及 $$\omega$$ 这个参数，但feature部分提到了“Handles missing values in the data seamlessly.”，说明似乎可以直接掺杂无标签数据
	- 发生错误“The above exception was the direct cause of the following exception”，发现是predict部分eval(X)没有转换成ndarray
- ## XGBoost
  id:: 63ff1e92-347e-4f13-ab2f-edcffae15b2d
  collapsed:: true
	- 别名: eXtreme Gradient Boosting
	- ((64468a02-5425-4d78-9616-62e848fbdc27))
- [xgboost原理？ - 知乎](https://www.zhihu.com/question/58883125/answer/206813653)
- [【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）](https://zhuanlan.zhihu.com/p/87885678 )
- [机器学习算法GBDT的面试要点总结-上篇 - ModifyBlog - 博客园](https://www.cnblogs.com/modifyrong/p/7744987.html)
- ## 梯度提升树
  id:: 63ff2001-a923-4190-977e-5e9317bbc236
  collapsed:: true
	- 别名: 梯度提升(决策)树, gradient boosting decision tree, gradient boosting trees, GB trees, GBDT
	- ((64468a02-5425-4d78-9616-62e848fbdc27))
- [梯度提升树(GBDT)原理小结 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/6140514.html)
- "GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。当然了，它里面的弱分类器的表现形式就是各棵树。" [NLP-LOVE (2021) GBDT](https://github.com/NLP-LOVE/ML-NLP/blob/fda5748094d086544a034b8e9df97a5953c0f608/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md )
- ## GBDT和XGBoost的区别
  collapsed:: true
	- > ((63ff2001-a923-4190-977e-5e9317bbc236))
	- > ((63ff1e92-347e-4f13-ab2f-edcffae15b2d))
- "首先xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。**xgboost里面的基学习器除了用tree(gbtree)，也可用线性分类器(gblinear)。**而GBDT则特指梯度提升决策树算法。"[机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎](https://www.zhihu.com/question/41354392/answer/98658997)
  id:: 612ee665-44cc-401b-93e4-274c38006124