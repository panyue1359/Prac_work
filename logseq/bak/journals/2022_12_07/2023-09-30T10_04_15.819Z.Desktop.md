- ## 随机森林
  id:: 63ff20bd-e600-4aec-ba91-c49c7d05ee68
  collapsed:: true
	- 别名: random forest, RF
	- ((64468a02-5425-4d78-9616-62e848fbdc27))
	  collapsed:: true
		- > ((63ff2165-f18c-49b6-90ca-c7654b8ad342))
- 随机森林是Bagging集成学习决策树增加了随机性
- 调参
  collapsed:: true
	- 根据 [scikit-learn随机森林调参小结 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/6160412.html) 确定调参顺序
	  id:: 6145bb3f-38c5-4a9e-bd0e-fdfcbedba05d
	- [sklearn对随机森林调参的建议](https://hyp.is/9wB_zjAKEeyiJKcPx5SlzQ/scikit-learn.org/stable/modules/ensemble.html)
	  :LOGBOOK:
	  CLOCK: [2021-10-18 Mon 20:03]
	  :END:
	- 网格搜索结果展示代码参考 [Python机器学习库sklearn网格搜索与交叉验证_Young_618-CSDN博客_sklearn 网格搜索](https://blog.csdn.net/cymy001/article/details/78578665)
	  id:: 61449dfe-c05f-4ce6-a5bd-74cb94a6c30b
	  collapsed:: true
		- ``` python
		  print('GridSearchCV交叉验证网格搜索字典获得的最好参数组合',
		  gsearch1.best_params_)
		  print('GridSearchCV交叉验证网格搜索获得的最好估计器,在训练验证集上没做交叉验证的得分',
		  gsearch1.score(X_trainval,y_trainval))
		  print('GridSearchCV交叉验证网格搜索获得的最好估计器,在**集上做交叉验证的平均得分',
		  gsearch1.best_score_)
		  print('GridSearchCV交叉验证网格搜索获得的最好估计器,在测试集上的得分',
		  gsearch1.score(X_test, y_test))
		  
		  ```
- ## 随机森林特征重要性排序
  collapsed:: true
	- ((63ff20bd-e600-4aec-ba91-c49c7d05ee68))
- 如果不控制随机数种子，那么随机森林特征重要性的排名可能会改变；如果改变很大，说明不同特征对响应变量的影响差别不大 [python feature selection feature importance method from sklearn.ensemble gives inconsistent results in multiple runs - Stack Overflow](https://stackoverflow.com/questions/57664705/python-feature-selection-feature-importance-method-from-sklearn-ensemble-gives-i)
- "Prediction from pure-prediction algorithm is accurate, not so much because of a specific/particular variable, but because of the interim non-linear transformation which includes many variables." 如果特征之间存在非线性相关性，那么去除其中一些变量不会损害模型表现 [Random forest importance measures are NOT important](https://eranraviv.com/random-forest-importance-measures-are-not-important/)
- 一个比较直观的例子是特征A是面积，特征B和C是长和宽，那么即使去掉特征A，也可以通过特征B和C凑出特征A [python - How to interpret feature importance for ensemble methods? - Stack Overflow](https://stackoverflow.com/questions/43637662/how-to-interpret-feature-importance-for-ensemble-methods)
- 基于不纯度降低的特征重要性只能说明模型使用各个特征的次数，“对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。”也就是说，随机森林特征重要性排序靠前的特征确实是重要特征，但排序靠后不一定不重要，即随机森林倾向于低估特征的重要性。[budomo a 特征选择——高级部分（线性模型正则化，随机森林、基于顶层模型的方法实现特征选择）](https://zhuanlan.zhihu.com/p/141704199)
- 当模型外推能力较差时，特征之间的相关性会降低置换重要性的可靠性 [为什么要停止过度使用置换重要性来寻找影响特征_deephub的博客-CSDN博客](https://blog.csdn.net/deephub/article/details/107971941?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167092444016782427446147%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=167092444016782427446147&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-10-107971941-null-null.142^v68^control,201^v4^add_ask,213^v2^t3_esquery_v2&utm_term=%E7%BD%AE%E6%8D%A2%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7&spm=1018.2226.3001.4187)、[[1905.03151] Unrestricted Permutation forces Extrapolation: Variable Importance Requires at least One More Model, or There Is No Free Variable Importance](https://arxiv.org/abs/1905.03151)
- “Main takeaway from the post: don’t use those importance scores plots, because they are simply misleading. Those importance plots are simply a wrong turn taken by our human tendency to look for reason, whether it’s there or it’s not there.” [Random forest importance measures are NOT important](https://eranraviv.com/random-forest-importance-measures-are-not-important/)