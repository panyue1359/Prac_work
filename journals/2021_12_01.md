- ## 特征缩放
  collapsed:: true
	- ((6423a4af-0897-4056-8058-5c5c3e5fb60c))
	- 别名: feature scaling
- 距离敏感的算法如KNN、SVM和PCA都需要特征缩放；对于有异常值的数据应该使用RobustScaler，其他方法（StandardScaler、MinMaxScaler和MaxAbsScaler都不合适）[DOTA a](https://zhuanlan.zhihu.com/p/400239115)
- "**如果对输出结果范围有要求，用归一化**; 如果数据较为稳定，不存在极端的最大最小值，用归一化; 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响. 一般来说，我个人建议优先使用标准化。"[特征工程中的「归一化」有什么作用？ - 知乎](https://www.zhihu.com/question/20455227)
- ## 正则化
  collapsed:: true
	- 别名：regularization
	- ((64468a02-5425-4d78-9616-62e848fbdc27))
	  collapsed:: true
		- collapsed:: true
		  >参考
			- ((63ff22ed-49a1-4c28-8035-c43735af92ca))
			- 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016.
- 原理："直译应该是：规则化（加个“化”字变动词，自豪一下中文还是强）。什么是规则？你妈喊你6点前回家吃饭，这就是规则，一个限制。同理，在这里，规则化就是说给需要训练的目标函数加上一些规则（限制），让他们不要自我膨胀。" 好比我丢了一卡通，如果在整个学校里地毯式的搜索，效率肯定很低，所以先划定一个范围比如食堂，然后去找。 [且行且安~ (2018) 正则化的理解 CSDN博客](https://blog.csdn.net/qq_20412595/article/details/81636105)
- “1）实现参数的稀疏有什么好处吗？  一个好处是可以**简化模型**，**避免过拟合**。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据表现性能极差。另一个好处是参数变少可以使整个模型获得更好的**可解释性**。”……“为什么参数越小，说明模型越简单呢，这是因为**越复杂的模型，越是会尝试对所有的样本进行拟合**，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生**较大的波动**，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。” [且行且安~ (2018) 正则化的理解 CSDN博客](https://blog.csdn.net/qq_20412595/article/details/81636105)
- L2和L1的区别：L1的特点是可以生成系数的特征向量，且多数权值为0 (周志华, 2016)