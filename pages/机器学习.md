别名:: machine learning, ML, artificial intelligence, AI

- # A
- [[AutoML]]
- # B
- [[半监督学习]]
- # D
- 读取csv文件：Excel读取csv有“/”的数据会自动填充，因此一定不能用Excel读取csv文件
- # F
- 分布外泛化
  id:: 63ac0dc5-b00e-4aab-b385-96ef02eee98c
  collapsed:: true
	- 别名:: out of distribution, OOD
- # K
- 可解释性
  id:: 63cf8d64-f6ae-4c89-9962-66b6cd8f1484
  collapsed:: true
	- 别名:: Interpretable machine learning
	- ## 本质的 (Intrinsic) VS 事后的 (Post-hoc)
	  collapsed:: true
		- ->Molnar C. 可解释机器学习：黑盒模型可解释性理解指南[M]. 朱明超, 译. 电子工业出版社, 2021.
	- ## 特定于模型 (Model-specific) VS 模型无关 (Model-agnostic)
	  collapsed:: true
		- >((63cf8d65-6c1d-4a18-b75d-2fc99b764221))
	- 特征重要性属于特定于模型的可解释性方法
	- ## 局部 (Local) VS 全局 (Global)
	  collapsed:: true
		- > [[特征选择]]
	- 特征选择方法属于全局可解释性
	- ## Shap
	  id:: f5e7a530-b10f-404c-8f6f-1e66281d6318
	  collapsed:: true
		- ->[特征工程：使用SHAP值构造类别特征和交叉特征 - 知乎](https://zhuanlan.zhihu.com/p/366022336)
- 可视化
  id:: 62903b40-9a51-43be-b6c6-89da210691c8
- # M
- [[模型评价指标]]
- # P
- 平台和语言
  collapsed:: true
	- ((9baa0f1a-1102-4ee5-aa94-6a68e36943cd))
	- [[Wolfram Mathematica]]
	- ((2c801976-20a7-4074-9555-84c855555d96))
	- ((cf406652-b83f-4f0c-b10b-7c2b6ee2ad27))
	- ((264e4e53-e838-4bf6-91ce-c8a663676adc))
- 偏差-方差分析
  collapsed:: true
	- 随训练集变化，训练结果变化很大，说明方差很大
	  id:: 61cfce7c-6bb8-4182-ab46-f907856f9c2e
- # R
- 入门建议
  collapsed:: true
	- 机器学习的理想是“高针对性数据加通用算法”（傅睿卿 a），在工程应用上数据是核心，算法是工具。因此入门机器学习应该直接从实践开始，遇到问题按需学习理论（微调 a），同时要注意学习资料的时效性（微调 a）。
	- 机器学习的理论需要通过编程实现，推荐学习Python语言，其次R语言（微调 a）。pytho的机器学习库中，做传统机器学习推荐使用scikit-learn，因为“sklearn主要适合 **中小型的** 、实用机器学习项目，尤其是那种**数据量不大**且需要使用者 **手动对数据进行处理** ，并选择合适模型的项目。这类项目往往在CPU上就可以完成，对硬件要求低。”（微调 c）。
	- 入门视频课程推荐Andrew Ng的课程和林轩田的课程（desperado a），书籍推荐S. Raschka (2017) 《python机器学习》。流行的西瓜书（周志华 (2016) 《机器学习》）更适合作为参考书看。（微调 a）
	- 如无必要，不建议从深度学习开始入手（微调 a），因为深度学习的模型和理论复杂而且还在变化中（微调 a, b），硬件要求高（微调 a），调参困难（微调 b），前置课程多（微调 b）。
	- > [傅睿卿 a 如何看待「机器学习不需要数学，很多算法封装好了，调个包就行」这种说法？](https://www.zhihu.com/question/60064269/answer/172522358)
	  [微调 a 如何用3个月零基础入门「机器学习」？ - 知乎](https://zhuanlan.zhihu.com/p/29704017)
	  [微调 b 深度学习的教学和课程，与传统 CS 的教学和课程有什么区别？](https://www.zhihu.com/question/63883727/answer/225499427)
	  [微调 c 现在tensorflow和mxnet很火，是否还有必要学习scikit-learn等框架？](https://www.zhihu.com/question/53740695/answer/284428668)
	  [desperado a 机器学习最好的课程是什么？](https://www.zhihu.com/question/37031588/answer/71271829)
	  [刘建平机器学习文章和代码](https://github.com/ljpzzz/machinelearning)
- # S
- 数据集的处理
  collapsed:: true
	- ## [[数据集不平衡]]
	  id:: 81580cd4-1797-4446-85fd-bf83dbad4b0e
	- ## 数据量的影响
	- “数据量很小，用朴素贝叶斯、逻辑回归或支持向量机；数据量适中或者较大，用树模型，优先 xgboost和lightgbm；数据量较大，尝试使用神经网络”[「机器学习」到底需要多少数据？ - 知乎](https://zhuanlan.zhihu.com/p/34523880)
	- ## 数据集划分
	  id:: 626a6929-a955-4dd7-a701-9608a85c19bd
	- "只有在同样的测试集上，两个（或以上）模型的对比才有效。".."所以，如果你的研究，是靠着比别人的模型效果来说事儿，那就一定先要弄明白对方的测试集是什么。"[如何正确使用机器学习中的训练集、验证集和测试集？ - 知乎](https://zhuanlan.zhihu.com/p/71961236)
	- ## 数据集可视化
	- Kirk[@Kirk2022]等使用t-SNE降维方法可视化AlCoCrFeMnNiV高熵合金候选样本，在t-SNE图中相近的点代表相近的成分，具体内容看不懂。
- [[随机森林]]
- # T
- 特征工程
  collapsed:: true
	- 别名:: feature engineering
	- 特征工程包括：[[降维]]、 [[特征选择]]、 [[特征缩放]]
- 特征抽取（feature extraction）：将声音、图像等非数字化信息转化成可用于机器学习的数字格式的技术
  id:: 61d8ef1a-fc2c-4465-b8ba-a84fdf104749
- 特征选择后如何区分不同的特征
  collapsed:: true
	- [如何在pandas中使用sklearn-fit_转换并返回dataframe而不是numpy数组？ - 问答 - Python中文网](https://www.cnpython.com/qa/31782)建议使用sklearn_pandas包的DataFrameMapper，使用方法类似于pipeline，但可以处理指定列和行
	  id:: 6176b00a-53b5-415b-adc4-ae56dc9e5c57
	- 处理后返回feature_names_in_属性值，重新组装成DataFrame
- # X
- 寻找最容易判断错误的样本（代码）
  collapsed:: true
	- ``` python
	  """
	  使用不同的随机数种子划分数据集，分析容易判错的样本
	  """
	  for i in range(11,20):
	      rege=RandomForestRegressor(random_state=0)
	      k=0
	      b=0
	      for _ in range(100,i*10,i):
	     X_trainval,X_test,y_trainval,y_test=train_test_split(foo_X,foo_y,test_size=0.2,random_state=_)
	     a=np.mean(cross_val_score(rege,X_trainval,y_trainval,cv=12))
	     #收集分数最低的随机数种子
	     if a<k:
	         k=a
	         b=_
	      X_trainval,X_test,y_trainval,y_test=train_test_split(foo_X,foo_y,test_size=0.2,random_state=b)
	      kf=KFold(n_splits=12)
	      testindex=np.array([])
	      for train_index, test_index in kf.split(X_trainval):
	     if np.all(test_index==np.array([0,1])):
	         testindex=test_index
	     else:
	         testindex=np.vstack((testindex,test_index))
	      cvscore=cross_val_score(rege,X_trainval,y_trainval,cv=12)
	      testindex[cvscore<0]=True
	      testindex[cvscore>=0]=False
	      print('对于随机数种子%s,其最容易判错的样本序号是：' %b,np.sort(X_trainval[np.array(testindex[testindex>-1],dtype=bool)].index))
	  
	  ```
	- i在1到9之间，随机数种子在0到i*10之间
	  collapsed:: true
		- 对于随机数种子1,其最容易判错的样本序号是： [ 1  3  8 12 22 24 26 27]
		  对于随机数种子2,其最容易判错的样本序号是： [ 2  3  5  8 15 17 20 27]
		  对于随机数种子15,其最容易判错的样本序号是： [ 1  2  3  6  8 10 21 24 26 29]
		  对于随机数种子20,其最容易判错的样本序号是： [ 2  3  6  7  9 11 14 16 17 19 20 22 23 24 25 26]
		  对于随机数种子15,其最容易判错的样本序号是： [ 1  2  3  6  8 10 21 24 26 29]
		  对于随机数种子30,其最容易判错的样本序号是： [ 1  2  3  5 12 14 16 18 19 20 23 29]
		  对于随机数种子56,其最容易判错的样本序号是： [ 1  6  8 19 24 25 27 28]
		  对于随机数种子56,其最容易判错的样本序号是： [ 1  6  8 19 24 25 27 28]
		  对于随机数种子72,其最容易判错的样本序号是： [ 7  8 12 13 16 18 19 24 28 29]
	- i在11到19之间，随机数种子在100到i*10之间
	  collapsed:: true
		- 对于随机数种子100,其最容易判错的样本序号是： [ 0  1  3  4  6  7  8 11 14 17 18 19 21 22 24 29]
		  对于随机数种子112,其最容易判错的样本序号是： [ 1  8  9 10 14 15 17 19 20 22 23 26]
		  对于随机数种子113,其最容易判错的样本序号是： [ 1  3  5  6  7 13 20 24 25 28]
		  对于随机数种子128,其最容易判错的样本序号是： [ 0  2  3  5 10 11 12 18 19 20 21 23 28 29]
		  对于随机数种子115,其最容易判错的样本序号是： [ 2  3  5  6  7 12 13 15 16 19 22 23 25 29]
		  对于随机数种子148,其最容易判错的样本序号是： [ 2  5  7 10 13 14 15 17 19 23 25 27 28 29]
		  对于随机数种子168,其最容易判错的样本序号是： [ 1  6  9 10 11 12 13 16 19 21 23 25]
		  对于随机数种子118,其最容易判错的样本序号是： [ 1  3  5  8 13 14 16 19 24 26]
		  对于随机数种子100,其最容易判错的样本序号是： [ 0  1  3  4  6  7  8 11 14 17 18 19 21 22 24 29]
	- 综上，最容易判错的样本是1，2，3，6，29，排除以后效果很差
	  collapsed:: true
		- 从foo_X和foo_y中排除特定的行
		  collapsed:: true
			- ``` python
			  #把要去掉的行换成Na，全是Na的行返回False，其余行返回True，最后使用索引过滤
			  foo_X.loc[[1,2,3,6,29]]=np.nan
			  foo_X=foo_X[foo_X.any(1)]
			  foo_y.loc[[1,2,3,6,29]]=np.nan
			  foo_y=foo_y[foo_y!=0]
			  
			  ```
			- 容易判错的数据多为FCC+BCC结构，增加FCC+BCC结构的数据
- # Y
- 异常检测
	- 别名:: anomaly detection, novelty detection,离群点检测, 异常检测, outlier detection
	- ## 异常值和噪声值
	  collapsed:: true
		- >Aggarwal C C. Outlier Analysis[M]. New York: Springer, 2013.
	- 理想情况下，异常点就是偏离正常模式的点；实际中，由于噪声的存在，不可能区分异常点和噪声点，因此噪声和异常只是概念上的区别。“Therefore, throughout this book, the term “outlier” refers to a data point that could either be considered an abnormality or noise, whereas an “anomaly” refers to a special kind of outlier that is of interest to an analyst.” (C.C. Aggarwal, 2013)
	- ## 工具箱
	  collapsed:: true
		- [微调](https://zhuanlan.zhihu.com/p/58313521)推荐：[A Python Toolbox for Scalable Outlier Detection (Anomaly Detection)](https://github.com/yzhao062/pyod)
	- ## 材料领域的异常检测
	  collapsed:: true
		- [[材料信息学]]
	- 主要用于图像缺陷检测